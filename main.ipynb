{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def drawProgressBar(shell_out, \n",
    "                    begin, k, out_of, end, barLen =25):\n",
    "    percent = k/float(out_of)\n",
    "    sys.stdout.write(\"\\r\")\n",
    "    progress = \"\"\n",
    "    for i in range(barLen):\n",
    "        if i < int(barLen * percent):\n",
    "            progress += \"=\"\n",
    "        elif i==int(barLen * percent):\n",
    "            progress +='>'\n",
    "        else:\n",
    "            progress += \"_\"\n",
    "    text = \"%s%d/%d[%s](%.2f%%)%s\"%(begin,k,out_of,progress,percent * 100, end)\n",
    "    if shell_out== True:\n",
    "        sys.stdout.write(text)\n",
    "        sys.stdout.flush()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/Simdiva/DSL-Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "languages = [\"bg\",\"mk\",\"bs\",\"hr\",\"sr\",\"cz\",\"sk\",\"es-ES\",\"es-AR\",\"pt-BR\",\"pt-PT\",\"id\",\"my\",\"xx\"]\n",
    "\n",
    "\n",
    "label_to_index ={k:v for v,k in enumerate(languages)} \n",
    "index_to_label = {k:v for k,v in enumerate(languages)}\n",
    "\n",
    "# print label_to_index['bs']\n",
    "# print index_to_label[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path= \"./DSL-Task/data/DSLCC-v2.0/train-dev/train.txt\"\n",
    "dev_path= \"./DSL-Task/data/DSLCC-v2.0/train-dev/devel.txt\"\n",
    "test_path = \"./DSL-Task/data/DSLCC-v2.0/test/test.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "def load(path):\n",
    "    with codecs.open(path,'rb','utf-8') as h:\n",
    "        content = h.read()\n",
    "    text_lines = content.split(\"\\n\")\n",
    "    return text_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def character_splitting(text):\n",
    "    char_list = list(text)\n",
    "    for i,ch in enumerate(char_list):\n",
    "        if ch == ' ':\n",
    "            char_list[i]='<sp>'\n",
    "    output = ' '.join(char_list)\n",
    "    return output\n",
    "#print character_splitting('This is a test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def preprocess_lines(text_lines):\n",
    "    text_labels = []\n",
    "    for text_line in text_lines:\n",
    "        if len(text_line)>0:\n",
    "            text_line = text_line.strip()\n",
    "            try:\n",
    "                text,label = text_line.split(\"\\t\")\n",
    "            except:\n",
    "                text = text_line\n",
    "                label = \"\"\n",
    "            text_labels.append((text,label))\n",
    "    return text_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_lines = load(train_path)\n",
    "dev_lines = load(dev_path)\n",
    "test_lines = load(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_samples: 252000\n",
      "dev_samples: 28000\n",
      "test_samples: 14000\n"
     ]
    }
   ],
   "source": [
    "train_samples = preprocess_lines(train_lines)\n",
    "dev_samples = preprocess_lines(dev_lines)\n",
    "test_samples = preprocess_lines(test_lines)\n",
    "print \"train_samples: %d\"%len(train_samples)\n",
    "print \"dev_samples: %d\"%len(dev_samples)\n",
    "print \"test_samples: %d\"%len(test_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Lookup Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 0\n",
    "SOS_token = n\n",
    "EOS_token = n+1\n",
    "PAD_token = n+2\n",
    "\n",
    "from collections import defaultdict\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = defaultdict(lambda: 0)\n",
    "        self.index2word = {}\n",
    "        self.index2word[SOS_token] = \"SOS\"\n",
    "        self.index2word[EOS_token] = \"EOS\"\n",
    "        self.index2word[PAD_token] = \"PAD\"\n",
    "        self.n_words = len(self.index2word) # Count all words \n",
    "      \n",
    "    def index_words(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.index_word(word)\n",
    "\n",
    "    def index_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "# lang = Lang('all')\n",
    "# lang.index_words('This is a test')\n",
    "# print \"number of words:%d\"%lang.n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of vocabularies= 1108619\n"
     ]
    }
   ],
   "source": [
    "lang = Lang('languages')\n",
    "for sample in train_samples +  dev_samples + test_samples:\n",
    "    text = sample[0]\n",
    "    lang.index_words(text)\n",
    "    \n",
    "print \"number of vocabularies= %d\"%lang.n_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace words of sentences with indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "MAX_TEXT_LEN = 5\n",
    "def indicies_from_sentence(lang, sentence):\n",
    "    output = [SOS_token]\n",
    "    output += [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    if len(output)+1 >= MAX_TEXT_LEN:\n",
    "        output = output[:MAX_TEXT_LEN-1]\n",
    "        output += [EOS_token]\n",
    "    else:\n",
    "        len_gap = MAX_TEXT_LEN - len(output) -1\n",
    "        pads = [PAD_token]*len_gap\n",
    "        output += [EOS_token]\n",
    "        output += pads\n",
    "    \n",
    "    return output\n",
    "\n",
    "def variable_from_sentence(lang, sentence):\n",
    "    indexes = indicies_from_sentence(lang, sentence)\n",
    "    var = Variable(torch.LongTensor(indexes).view(-1, 1))\n",
    "    return var\n",
    "\n",
    "def variable_from_label(label):\n",
    "    return Variable(torch.LongTensor([label_to_index[label]])).view(-1,1)\n",
    "\n",
    "#print \"indicies are:\", indicies_from_sentence(lang,'this is')\n",
    "# print \"variable is:\", variable_from_sentence(lang,'this is')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class  CNN_Text(nn.Module):\n",
    "    def __init__(self,params):\n",
    "        super(CNN_Text,self).__init__()\n",
    "        \n",
    "        V = params['embed_num']\n",
    "        D = params['embed_dim']\n",
    "        C = params['class_num']\n",
    "        Ci = 1\n",
    "        Co = params['kernel_num']\n",
    "        Ks = params['kernel_sizes']\n",
    "\n",
    "        self.embed = nn.Embedding(V, D)\n",
    "        \n",
    "        self.convs1 = nn.ModuleList([nn.Conv2d(Ci, Co, (K, D)) for K in Ks])\n",
    "        '''\n",
    "        self.conv13 = nn.Conv2d(Ci, Co, (3, D))\n",
    "        self.conv14 = nn.Conv2d(Ci, Co, (4, D))\n",
    "        self.conv15 = nn.Conv2d(Ci, Co, (5, D))\n",
    "        '''\n",
    "        self.dropout = nn.Dropout(params['dropout'])\n",
    "        self.fc1 = nn.Linear(len(Ks)*Co, C)\n",
    "\n",
    "    def conv_and_pool(self, x, conv):\n",
    "        x = F.relu(conv(x)).squeeze(3) #(N,Co,W)\n",
    "        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embed(x) # (N,W,D)\n",
    "        \n",
    "        x = x.unsqueeze(1) # (N,Ci,W,D)\n",
    "\n",
    "        x = [F.relu(conv(x)).squeeze(3) for conv in self.convs1] #[(N,Co,W), ...]*len(Ks)\n",
    "\n",
    "\n",
    "        x = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in x] #[(N,Co), ...]*len(Ks)\n",
    "\n",
    "        x = torch.cat(x, 1)\n",
    "\n",
    "        '''\n",
    "        x1 = self.conv_and_pool(x,self.conv13) #(N,Co)\n",
    "        x2 = self.conv_and_pool(x,self.conv14) #(N,Co)\n",
    "        x3 = self.conv_and_pool(x,self.conv15) #(N,Co)\n",
    "        x = torch.cat((x1, x2, x3), 1) # (N,len(Ks)*Co)\n",
    "        '''\n",
    "        x = self.dropout(x) # (N,len(Ks)*Co)\n",
    "        logit = self.fc1(x) # (N,C)\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "def get_batches(data_samples, batch_size):\n",
    "    random.shuffle(data_samples)\n",
    "    \n",
    "    n =  len(data_samples)\n",
    "    num_batches = n / batch_size\n",
    "    data_samples = data_samples[:num_batches*batch_size]\n",
    "    batches =[]\n",
    "    for k in range(num_batches):\n",
    "        batch = data_samples[k*batch_size:(k+1)*batch_size]\n",
    "        batch_texts = [variable_from_sentence(lang,sample[0]) for sample in batch]\n",
    "        batch_labels = [variable_from_label(sample[1]) for sample in batch]\n",
    "       \n",
    "        batch_texts = torch.stack(batch_texts,dim=0)\n",
    "        batch_labels = torch.stack(batch_labels,dim=0)\n",
    "       \n",
    "        batches.append((batch_texts,batch_labels))\n",
    "    return batches\n",
    "\n",
    "# b0 =  get_batches(train_samples,5)[0]\n",
    "# b0_texts ,b0_labels = b0[0],b0[1]\n",
    "# print torch.stack(b0_labels).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def eval(data_samples, model,batch_size):\n",
    "    \n",
    "    corrects, avg_loss = 0, 0\n",
    "    \n",
    "    data_batches = get_batches(data_samples, batch_size)\n",
    "        \n",
    "    for bi,batch in enumerate(data_batches):\n",
    "        input_var, label_var = batch[0], batch[1] \n",
    "        \n",
    "        input_var = input_var.view(batch_size,-1)\n",
    "        \n",
    "        label_var =label_var.view(batch_size)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            input_var, label_var = input_var.cuda(), label_var.cuda()\n",
    "\n",
    "        predictions = model(input_var)\n",
    "        loss = F.cross_entropy(predictions, label_var, size_average=False)\n",
    "\n",
    "        avg_loss += loss.data[0]\n",
    "        corrects += (torch.max(predictions, 1)\n",
    "                     [1].view(label_var.size()).data == label_var.data).sum()\n",
    "\n",
    "    size = len(data_samples)\n",
    "    avg_loss = avg_loss/size\n",
    "    accuracy = 100.0 * corrects/size\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1, 312/312[=========================](100.00%) loss:2.1829 train_acc:44.82, dev_acc:29.00\n",
      "\n",
      "epoch:6, 312/312[=========================](100.00%) loss:1.0216 train_acc:95.13, dev_acc:36.00\n",
      "\n",
      "epoch:11, 312/312[=========================](100.00%) loss:0.5968 train_acc:98.58, dev_acc:39.00\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-eeff449cc001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mcnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-eeff449cc001>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_samples, dev_samples, model)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                  \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-bd5fe31eca9b>\u001b[0m in \u001b[0;36meval\u001b[0;34m(data_samples, model, batch_size)\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0minput_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/mesgarmn/anaconda2/lib/python2.7/site-packages/torch/nn/modules/module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-8b8ebffebde4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#[(N,Co), ...]*len(Ks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         '''\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "def train(train_samples, dev_samples, model):\n",
    "    \n",
    "    batch_size = 32\n",
    "    \n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "    \n",
    "    num_epochs =25\n",
    "    \n",
    "    steps = 0\n",
    "    \n",
    "    train_batches = get_batches(train_samples,batch_size)\n",
    "    \n",
    "    epoch_loss =0.0\n",
    "    train_acc = 0.0\n",
    "    dev_acc = 0.0\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        \n",
    "        for bi,batch in enumerate(train_batches):\n",
    "\n",
    "            input_var, label_var = batch[0], batch[1] \n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                input_var, label_var = input_var.cuda(), label_var.cuda()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_var = input_var.view(batch_size,-1)\n",
    "            \n",
    "            predictions = model(input_var)\n",
    "\n",
    "            \n",
    "            label_var =label_var.view(batch_size)\n",
    "           \n",
    "            loss = F.cross_entropy(predictions, label_var)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.data[0]\n",
    "            \n",
    "            steps += 1\n",
    "            \n",
    "\n",
    "            if steps % 20 == 0:\n",
    "                 train_acc = eval(train_samples,model,1)\n",
    "           \n",
    "            if steps % 20 == 0:\n",
    "                dev_acc = eval(dev_samples, model,1)\n",
    "                \n",
    "            if (epoch-1) % 5 ==0:\n",
    "                drawProgressBar(True,'epoch:%d, '%(epoch),bi+1, len(train_batches),' loss:%.4f train_acc:%.2f, dev_acc:%.2f'%(\n",
    "                                        epoch_loss/float(steps),\n",
    "                                        train_acc,\n",
    "                                        dev_acc))\n",
    "        if (epoch-1) % 5 ==0:\n",
    "            print \"\\n\"\n",
    "            \n",
    "\n",
    "params = {}\n",
    "params['embed_num'] = lang.n_words\n",
    "params['embed_dim'] = 200\n",
    "params['class_num'] = len(languages)\n",
    "params['kernel_num'] = 100\n",
    "params['kernel_sizes']=[3,4,5]\n",
    "params['dropout']=0.5\n",
    "cnn = CNN_Text(params)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cnn = cnn.cuda()\n",
    "        \n",
    "train(train_samples[:10000], dev_samples[100:200], cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
